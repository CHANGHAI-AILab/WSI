{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21b469b",
   "metadata": {},
   "source": [
    "## 获取待提取特征的文件\n",
    "\n",
    "提供两种批量处理的模式：\n",
    "1. 目录模式，提取指定目录下的所有jpg文件的特征。\n",
    "2. 文件模式，待提取的数据存储在文件中，每行一个样本。\n",
    "\n",
    "当然也可以在最后自己指定手动提取指定若干文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dee942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928972\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from onekey_algo import get_param_in_cwd\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "# 目录模式\n",
    "#data_root = os.path.join(get_param_in_cwd('data_root'), 'patches')\n",
    "data_root = r'F:\\TCGA-NORMAL\\patches'\n",
    "#data_root = r'E:\\SYX\\patchs'\n",
    "\n",
    "#samples = glob(os.path.join(data_root, '*', '*.jpg'))\n",
    "samples = glob(os.path.join(data_root, '*', '*.jpg'))\n",
    "print(len(samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26847144",
   "metadata": {},
   "source": [
    "## 确定提取特征\n",
    "\n",
    "通过关键词获取要提取那一层的特征。\n",
    "\n",
    "### 支持的模型名称\n",
    "\n",
    "模型名称替换代码中的 `model_name`变量的值。\n",
    "\n",
    "| **模型系列** | **模型名称**                                                 |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| AlexNet      | alexnet                                                      |\n",
    "| VGG          | vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19 |\n",
    "| ResNet       | resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2 |\n",
    "| DenseNet     | densenet121, densenet169, densenet201, densenet161           |\n",
    "| Inception    | googlenet, inception_v3                                      |\n",
    "| SqueezeNet   | squeezenet1_0, squeezenet1_1                                 |\n",
    "| ShuffleNetV2 | shufflenet_v2_x2_0, shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5 |\n",
    "| MobileNet    | mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small         |\n",
    "| MNASNet      | mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8d607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 15:19:28 - simple_vit.py:  93]\tINFO\t正在使用 SimpleViT，具体参数为：images_size=256, patch_size=32, num_classes=2, dim=1024, depth=6, heads=16, mlp_dim=2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 22222222222\n",
      "cuda\n",
      "device cuda\n",
      "Feature name:  || Module: SimpleViT(\n",
      "  (to_patch_embedding): Sequential(\n",
      "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
      "    (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU()\n",
      "            (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU()\n",
      "            (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU()\n",
      "            (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU()\n",
      "            (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU()\n",
      "            (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU()\n",
      "            (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_latent): Identity()\n",
      "  (linear_head): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "Feature name: to_patch_embedding || Module: Sequential(\n",
      "  (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
      "  (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "  (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Feature name: to_patch_embedding.0 || Module: Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
      "Feature name: to_patch_embedding.1 || Module: LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: to_patch_embedding.2 || Module: Linear(in_features=3072, out_features=1024, bias=True)\n",
      "Feature name: to_patch_embedding.3 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer || Module: Transformer(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU()\n",
      "          (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU()\n",
      "          (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU()\n",
      "          (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU()\n",
      "          (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU()\n",
      "          (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU()\n",
      "          (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers || Module: ModuleList(\n",
      "  (0): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU()\n",
      "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      ")\n",
      "Feature name: transformer.layers.0.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.0.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.0.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.0.0.to_out || Module: Linear(in_features=1024, out_features=1024, bias=False)\n",
      "Feature name: transformer.layers.0.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU()\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n",
      "Feature name: transformer.layers.0.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.0.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.0.1.net.2 || Module: GELU()\n",
      "Feature name: transformer.layers.0.1.net.3 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.1 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.1.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      ")\n",
      "Feature name: transformer.layers.1.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.1.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.1.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.1.0.to_out || Module: Linear(in_features=1024, out_features=1024, bias=False)\n",
      "Feature name: transformer.layers.1.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU()\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.1.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n",
      "Feature name: transformer.layers.1.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.1.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.1.1.net.2 || Module: GELU()\n",
      "Feature name: transformer.layers.1.1.net.3 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.2 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.2.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      ")\n",
      "Feature name: transformer.layers.2.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.2.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.2.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.2.0.to_out || Module: Linear(in_features=1024, out_features=1024, bias=False)\n",
      "Feature name: transformer.layers.2.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU()\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.2.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n",
      "Feature name: transformer.layers.2.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.2.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.2.1.net.2 || Module: GELU()\n",
      "Feature name: transformer.layers.2.1.net.3 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.3 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.3.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      ")\n",
      "Feature name: transformer.layers.3.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.3.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.3.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.3.0.to_out || Module: Linear(in_features=1024, out_features=1024, bias=False)\n",
      "Feature name: transformer.layers.3.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU()\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.3.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n",
      "Feature name: transformer.layers.3.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.3.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.3.1.net.2 || Module: GELU()\n",
      "Feature name: transformer.layers.3.1.net.3 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.4 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.4.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      ")\n",
      "Feature name: transformer.layers.4.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.4.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.4.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.4.0.to_out || Module: Linear(in_features=1024, out_features=1024, bias=False)\n",
      "Feature name: transformer.layers.4.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU()\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.4.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n",
      "Feature name: transformer.layers.4.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.4.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.4.1.net.2 || Module: GELU()\n",
      "Feature name: transformer.layers.4.1.net.3 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.5 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.5.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
      ")\n",
      "Feature name: transformer.layers.5.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.5.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.5.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.5.0.to_out || Module: Linear(in_features=1024, out_features=1024, bias=False)\n",
      "Feature name: transformer.layers.5.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU()\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.5.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU()\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n",
      "Feature name: transformer.layers.5.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.5.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.5.1.net.2 || Module: GELU()\n",
      "Feature name: transformer.layers.5.1.net.3 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: to_latent || Module: Identity()\n",
      "Feature name: linear_head || Module: Linear(in_features=1024, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from onekey_algo.custom.components.comp2 import extract, print_feature_hook, reg_hook_on_module, \\\n",
    "    init_from_model, init_from_onekey\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "#model, transformer, device = init_from_onekey(r'F:\\20230915-BianYun\\models\\SimpleViT\\viz')\n",
    "\n",
    "model, transformer, device = init_from_onekey(r'D:\\bianyun_onekey\\models\\SimpleViT\\viz')\n",
    "print('device',device)\n",
    "for n, m in model.named_modules():\n",
    "    print('Feature name:', n, \"|| Module:\", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f29370",
   "metadata": {},
   "source": [
    "## 提取特征\n",
    "\n",
    "`Feature name:` 之后的名称为要提取的特征名，例如`layer3.0.conv2`, 一般深度学习特征提取最后一层，例如`avgpool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6394dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "print(device)\n",
    "feature_name = 'to_latent'\n",
    "with open(os.path.join(r'D:\\bianyun_onekey\\models\\SimpleViT\\viz', f'dlfeature.csv'), 'w') as outfile:\n",
    "    hook = partial(print_feature_hook, fp=outfile)\n",
    "    find_num = reg_hook_on_module(feature_name, model, hook)\n",
    "    results = extract(samples, model, transformer, device, fp=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1285696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
